{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbMTsHPwI4imKW4xOxYuBw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ErangaOttachchige/Final-Year-Research-Project/blob/main/02_stage1_detection_filter_training_OPTIMIZED_for_Colab_Free_T4_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 1 Detection/Filter - ULTRA OPTIMIZED for Colab Free T4\n",
        "##\n",
        "### KEY OPTIMIZATIONS:\n",
        "### 1. Parallel caching with progress (8 workers)\n",
        "### 2. Reduced batch processing with gradient accumulation\n",
        "### 3. Smart worker config (no RAM crash)\n",
        "### 4. Optional: Skip caching, train directly from Drive (for smaller datasets)\n",
        "### 5. Memory-efficient data loading\n",
        "### 6. Faster evaluation"
      ],
      "metadata": {
        "id": "FbQL3LApQKut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SETUP: Mount Drive + Paths\n",
        "# ============================================================================\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import os\n",
        "DRIVE_CCT = \"/content/drive/MyDrive/datasets/cct20\"\n",
        "PROC_DIR  = f\"{DRIVE_CCT}/processed\"\n",
        "CSV_STAGE1 = f\"{PROC_DIR}/cct20_stage1_imagelevel.csv\"\n",
        "\n",
        "print(\"âœ“ PROC_DIR files:\", os.listdir(PROC_DIR))\n",
        "print(\"âœ“ CSV exists:\", os.path.exists(CSV_STAGE1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bArck8gIQIOE",
        "outputId": "ec14d7e1-d283-4a6e-d244-e221db08e295"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ“ PROC_DIR files: ['cct20_species_annotations.csv', 'cct20_stage1_imagelevel.csv', 'cct20_stage2_species_imagelevel.csv', 'stage2_best_species_efficientnet_b0_optimized.pt', 'stage2_label_mapping.json', 'stage1_best_efficientnet_b0.pt']\n",
            "âœ“ CSV exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# INSTALL PACKAGES\n",
        "# ============================================================================\n",
        "!pip -q install timm torchmetrics pandas numpy scikit-learn pillow tqdm\n",
        "\n",
        "import torch\n",
        "print(\"âœ“ CUDA:\", torch.cuda.is_available())\n",
        "print(\"âœ“ GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWRLvc6aQIV1",
        "outputId": "64ba2178-4847-4e20-c8ec-55bf22f206ea"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ CUDA: True\n",
            "âœ“ GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# LOAD CSV + VERIFY\n",
        "# ============================================================================\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(CSV_STAGE1)\n",
        "\n",
        "missing = (~df[\"path\"].apply(os.path.exists)).sum()\n",
        "print(f\"âœ“ Rows: {len(df)}, Missing paths: {missing}\")\n",
        "print(\"\\nSplit counts:\\n\", df[\"split\"].value_counts())\n",
        "print(\"\\nLabel counts:\\n\", df[\"label_stage1\"].value_counts())\n",
        "\n",
        "# Label mapping\n",
        "classes = sorted(df[\"label_stage1\"].unique())\n",
        "class_to_idx = {c: i for i, c in enumerate(classes)}\n",
        "idx_to_class = {i: c for c, i in class_to_idx.items()}\n",
        "df[\"y\"] = df[\"label_stage1\"].map(class_to_idx)\n",
        "\n",
        "print(f\"\\nâœ“ Classes: {classes}\")\n",
        "print(f\"âœ“ Num classes: {len(classes)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_iPhoAyQIZ9",
        "outputId": "24b7f511-55d1-40f8-9896-fb9bcd473ac9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Rows: 57864, Missing paths: 0\n",
            "\n",
            "Split counts:\n",
            " split\n",
            "test_trans    23275\n",
            "test_cis      15827\n",
            "train         13553\n",
            "val_cis        3484\n",
            "val_trans      1725\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Label counts:\n",
            " label_stage1\n",
            "animal    51237\n",
            "empty      4014\n",
            "car        2613\n",
            "Name: count, dtype: int64\n",
            "\n",
            "âœ“ Classes: ['animal', 'car', 'empty']\n",
            "âœ“ Num classes: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ULTRA-FAST PARALLEL CACHING (3-5 min instead of 80 min)\n",
        "# ============================================================================\n",
        "import glob, hashlib\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from functools import partial\n",
        "\n",
        "CACHE_DIR = \"/content/stage1_cache_224\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "def cache_path(img_path):\n",
        "    \"\"\"Generate cache filename\"\"\"\n",
        "    return os.path.join(CACHE_DIR, hashlib.md5(img_path.encode()).hexdigest() + \".pt\")\n",
        "\n",
        "df[\"cache_path\"] = df[\"path\"].apply(cache_path)\n",
        "\n",
        "cached_count = len(glob.glob(CACHE_DIR + \"/*.pt\"))\n",
        "print(f\"\\nâœ“ Cached tensors: {cached_count} / {len(df)}\")\n",
        "\n",
        "# Preprocessing transform\n",
        "pre_tf = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "def process_one_image(row):\n",
        "    \"\"\"Process and cache a single image (thread-safe)\"\"\"\n",
        "    cp = row[\"cache_path\"]\n",
        "    if os.path.exists(cp):\n",
        "        return\n",
        "    try:\n",
        "        img = Image.open(row[\"path\"]).convert(\"RGB\")\n",
        "        x = pre_tf(img)\n",
        "        torch.save(x, cp)\n",
        "    except Exception as e:\n",
        "        # Silently skip corrupted images\n",
        "        pass\n",
        "\n",
        "if cached_count < len(df) * 0.95:\n",
        "    print(\"ðŸ”„ Parallel caching (8 workers, ~3-8 min depending on Drive speed)...\")\n",
        "\n",
        "    rows_to_process = [row for _, row in df.iterrows()]\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "        list(tqdm(\n",
        "            executor.map(process_one_image, rows_to_process),\n",
        "            total=len(rows_to_process),\n",
        "            desc=\"Caching Stage1\"\n",
        "        ))\n",
        "\n",
        "    final_count = len(glob.glob(CACHE_DIR + \"/*.pt\"))\n",
        "    print(f\"âœ“ Cache done: {final_count} tensors\")\n",
        "\n",
        "    # Update cache paths (some may have failed)\n",
        "    df[\"has_cache\"] = df[\"cache_path\"].apply(os.path.exists)\n",
        "    df = df[df[\"has_cache\"]].reset_index(drop=True)\n",
        "    print(f\"âœ“ Usable images: {len(df)}\")\n",
        "else:\n",
        "    print(\"âœ“ Cache exists - skipping preprocessing!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjoUb27LQIcV",
        "outputId": "17c1271b-1194-497a-d779-f515b5ed9d3f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ“ Cached tensors: 57864 / 57864\n",
            "âœ“ Cache exists - skipping preprocessing!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MEMORY-EFFICIENT DATASET (loads cached tensors)\n",
        "# ============================================================================\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "\n",
        "# Augmentation (applied to cached tensors during training)\n",
        "aug_tf = T.Compose([\n",
        "    T.RandomHorizontalFlip(0.5),\n",
        "    T.ColorJitter(0.2, 0.2, 0.1),\n",
        "])\n",
        "\n",
        "class CachedDataset(Dataset):\n",
        "    \"\"\"Loads preprocessed tensors (very fast)\"\"\"\n",
        "    def __init__(self, frame, augment=False):\n",
        "        self.df = frame.reset_index(drop=True)\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # Load cached tensor (super fast!)\n",
        "        x = torch.load(row[\"cache_path\"])\n",
        "\n",
        "        # Apply augmentation if training\n",
        "        if self.augment:\n",
        "            x = aug_tf(x)\n",
        "\n",
        "        y = int(row[\"y\"])\n",
        "        return x, y\n",
        "\n",
        "# Split data\n",
        "train_df = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
        "val_df   = df[df[\"split\"] == \"val_cis\"].reset_index(drop=True)\n",
        "valT_df  = df[df[\"split\"] == \"val_trans\"].reset_index(drop=True)\n",
        "test_cis_df   = df[df[\"split\"] == \"test_cis\"].reset_index(drop=True)\n",
        "test_trans_df = df[df[\"split\"] == \"test_trans\"].reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nâœ“ Splits:\")\n",
        "print(f\"  train: {len(train_df)}\")\n",
        "print(f\"  val_cis: {len(val_df)}\")\n",
        "print(f\"  val_trans: {len(valT_df)}\")\n",
        "print(f\"  test_cis: {len(test_cis_df)}\")\n",
        "print(f\"  test_trans: {len(test_trans_df)}\")\n",
        "\n",
        "# Create datasets\n",
        "train_ds = CachedDataset(train_df, augment=True)\n",
        "val_ds   = CachedDataset(val_df, augment=False)\n",
        "valT_ds  = CachedDataset(valT_df, augment=False)\n",
        "test_cis_ds   = CachedDataset(test_cis_df, augment=False)\n",
        "test_trans_ds = CachedDataset(test_trans_df, augment=False)\n",
        "\n",
        "# Balanced sampling for training\n",
        "counts = train_df[\"y\"].value_counts().sort_index()\n",
        "w_class = 1.0 / counts\n",
        "w_sample = train_df[\"y\"].map(w_class).values\n",
        "sampler = WeightedRandomSampler(\n",
        "    torch.tensor(w_sample, dtype=torch.double),\n",
        "    num_samples=len(w_sample),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "# Class weights for loss (FIXED: ensure all classes are represented)\n",
        "# Create a weight array for ALL classes (not just those in train set)\n",
        "class_weight = torch.ones(len(classes), dtype=torch.float32)\n",
        "\n",
        "for class_idx in counts.index:\n",
        "    class_weight[class_idx] = counts.sum() / (len(counts) * counts[class_idx])\n",
        "\n",
        "print(\"\\nâœ“ Train class distribution:\")\n",
        "print(counts)\n",
        "print(\"\\nâœ“ Loss class weights:\")\n",
        "print(class_weight)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKYUOkTGQIfO",
        "outputId": "48813cb1-b4c4-44b5-e728-a0de1f614984"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ“ Splits:\n",
            "  train: 13553\n",
            "  val_cis: 3484\n",
            "  val_trans: 1725\n",
            "  test_cis: 15827\n",
            "  test_trans: 23275\n",
            "\n",
            "âœ“ Train class distribution:\n",
            "y\n",
            "0    12885\n",
            "1      668\n",
            "Name: count, dtype: int64\n",
            "\n",
            "âœ“ Loss class weights:\n",
            "tensor([ 0.5259, 10.1445,  1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# OPTIMIZED DATALOADERS (no RAM crash)\n",
        "# ============================================================================\n",
        "BATCH_TRAIN = 32   # Smaller batch + gradient accumulation\n",
        "BATCH_EVAL  = 64\n",
        "NUM_WORKERS = 2    # CRITICAL: max 2 workers on Colab free\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_TRAIN,\n",
        "    sampler=sampler,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=False  # Disable for Colab stability\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=BATCH_EVAL,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=False\n",
        ")\n",
        "\n",
        "valT_loader = DataLoader(\n",
        "    valT_ds,\n",
        "    batch_size=BATCH_EVAL,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=False\n",
        ")\n",
        "\n",
        "test_cis_loader = DataLoader(\n",
        "    test_cis_ds,\n",
        "    batch_size=BATCH_EVAL,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=False\n",
        ")\n",
        "\n",
        "test_trans_loader = DataLoader(\n",
        "    test_trans_ds,\n",
        "    batch_size=BATCH_EVAL,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=False\n",
        ")"
      ],
      "metadata": {
        "id": "mvsBHHNVQIhu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TRAINING WITH GRADIENT ACCUMULATION + AMP\n",
        "# ============================================================================\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"\\nâœ“ Device: {device}\")\n",
        "\n",
        "# Model\n",
        "model = timm.create_model(\"efficientnet_b0\", pretrained=True, num_classes=len(classes)).to(device)\n",
        "\n",
        "# Loss + optimizer\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weight.to(device))\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
        "\n",
        "# AMP scaler\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device == \"cuda\"))\n",
        "\n",
        "# Gradient accumulation (effective batch = 32 * 2 = 64)\n",
        "ACCUM_STEPS = 2\n",
        "\n",
        "def eval_loader(loader, name=\"eval\"):\n",
        "    \"\"\"Fast evaluation with progress bar\"\"\"\n",
        "    model.eval()\n",
        "    all_y, all_p = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in tqdm(loader, desc=name, leave=False):\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            y = y.to(device, non_blocking=True)\n",
        "\n",
        "            with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n",
        "                logits = model(x)\n",
        "\n",
        "            preds = logits.argmax(1)\n",
        "            all_y.extend(y.cpu().tolist())\n",
        "            all_p.extend(preds.cpu().tolist())\n",
        "\n",
        "    acc = accuracy_score(all_y, all_p)\n",
        "    mf1 = f1_score(all_y, all_p, average=\"macro\")\n",
        "\n",
        "    return acc, mf1, all_y, all_p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9RUpmQoQIkV",
        "outputId": "8b24a7f3-5e84-43a5-870c-cbcbf4af10cd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ“ Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2929904351.py:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device == \"cuda\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TRAINING LOOP\n",
        "# ============================================================================\n",
        "SAVE_PATH = f\"{PROC_DIR}/stage1_best_efficientnet_b0.pt\"\n",
        "best_val_mf1 = -1.0\n",
        "EPOCHS = 5\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"TRAINING STAGE 1 - {EPOCHS} EPOCHS\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(pbar):\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        # Forward pass with AMP\n",
        "        with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y) / ACCUM_STEPS\n",
        "\n",
        "        # Backward pass\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Update weights every ACCUM_STEPS\n",
        "        if (batch_idx + 1) % ACCUM_STEPS == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        running_loss += loss.item() * x.size(0) * ACCUM_STEPS\n",
        "        pbar.set_postfix({\"loss\": f\"{loss.item() * ACCUM_STEPS:.4f}\"})\n",
        "\n",
        "    # Epoch metrics\n",
        "    train_loss = running_loss / len(train_df)\n",
        "\n",
        "    # Validate\n",
        "    val_acc, val_mf1, _, _ = eval_loader(val_loader, \"val_cis\")\n",
        "\n",
        "    print(f\"\\nEpoch {epoch}:\")\n",
        "    print(f\"  train_loss = {train_loss:.4f}\")\n",
        "    print(f\"  val_cis    = acc {val_acc:.3f}, macroF1 {val_mf1:.3f}\")\n",
        "\n",
        "    # Optional: validate on trans\n",
        "    if len(valT_df) > 0:\n",
        "        vt_acc, vt_mf1, _, _ = eval_loader(valT_loader, \"val_trans\")\n",
        "        print(f\"  val_trans  = acc {vt_acc:.3f}, macroF1 {vt_mf1:.3f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_mf1 > best_val_mf1:\n",
        "        best_val_mf1 = val_mf1\n",
        "        torch.save(model.state_dict(), SAVE_PATH)\n",
        "        print(f\"  ðŸ’¾ SAVED BEST (macroF1 = {best_val_mf1:.3f})\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-XNgg-PQIm8",
        "outputId": "39b060dc-d021-473e-bb07-86477852cd72"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TRAINING STAGE 1 - 5 EPOCHS\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/5:   0%|          | 0/424 [00:00<?, ?it/s]/tmp/ipython-input-1038353297.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n",
            "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 424/424 [01:25<00:00,  4.98it/s, loss=0.0000]\n",
            "val_cis:   0%|          | 0/55 [00:00<?, ?it/s]/tmp/ipython-input-2929904351.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1:\n",
            "  train_loss = 0.1315\n",
            "  val_cis    = acc 0.713, macroF1 0.505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "val_trans:   0%|          | 0/27 [00:00<?, ?it/s]/tmp/ipython-input-2929904351.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  val_trans  = acc 0.784, macroF1 0.294\n",
            "  ðŸ’¾ SAVED BEST (macroF1 = 0.505)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5:   0%|          | 0/424 [00:00<?, ?it/s]/tmp/ipython-input-1038353297.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n",
            "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 424/424 [01:22<00:00,  5.14it/s, loss=0.0000]\n",
            "val_cis:   0%|          | 0/55 [00:00<?, ?it/s]/tmp/ipython-input-2929904351.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2:\n",
            "  train_loss = 0.0019\n",
            "  val_cis    = acc 0.709, macroF1 0.458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "val_trans:   0%|          | 0/27 [00:00<?, ?it/s]/tmp/ipython-input-2929904351.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  val_trans  = acc 0.688, macroF1 0.273\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5:   0%|          | 0/424 [00:00<?, ?it/s]/tmp/ipython-input-1038353297.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n",
            "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 424/424 [01:23<00:00,  5.10it/s, loss=0.0000]\n",
            "val_cis:   0%|          | 0/55 [00:00<?, ?it/s]/tmp/ipython-input-2929904351.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3:\n",
            "  train_loss = 0.0003\n",
            "  val_cis    = acc 0.712, macroF1 0.506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "val_trans:   0%|          | 0/27 [00:00<?, ?it/s]/tmp/ipython-input-2929904351.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  val_trans  = acc 0.839, macroF1 0.305\n",
            "  ðŸ’¾ SAVED BEST (macroF1 = 0.506)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5:   0%|          | 0/424 [00:00<?, ?it/s]/tmp/ipython-input-1038353297.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n",
            "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 424/424 [01:25<00:00,  4.97it/s, loss=0.0000]\n",
            "val_cis:   0%|          | 0/55 [00:00<?, ?it/s]/tmp/ipython-input-2929904351.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4:\n",
            "  train_loss = 0.0002\n",
            "  val_cis    = acc 0.714, macroF1 0.525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "val_trans:   0%|          | 0/27 [00:00<?, ?it/s]/tmp/ipython-input-2929904351.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  val_trans  = acc 0.927, macroF1 0.321\n",
            "  ðŸ’¾ SAVED BEST (macroF1 = 0.525)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5:   0%|          | 0/424 [00:00<?, ?it/s]/tmp/ipython-input-1038353297.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n",
            "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 424/424 [01:23<00:00,  5.11it/s, loss=0.0000]\n",
            "val_cis:   0%|          | 0/55 [00:00<?, ?it/s]/tmp/ipython-input-2929904351.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5:\n",
            "  train_loss = 0.0006\n",
            "  val_cis    = acc 0.714, macroF1 0.532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "val_trans:   0%|          | 0/27 [00:00<?, ?it/s]/tmp/ipython-input-2929904351.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  val_trans  = acc 0.888, macroF1 0.314\n",
            "  ðŸ’¾ SAVED BEST (macroF1 = 0.532)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FINAL TEST EVALUATION\n",
        "# ============================================================================\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"FINAL TEST EVALUATION (Stage 1)\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load(SAVE_PATH, map_location=device))\n",
        "\n",
        "# Evaluate\n",
        "cis_acc, cis_mf1, cis_y, cis_p = eval_loader(test_cis_loader, \"test_cis\")\n",
        "tr_acc, tr_mf1, tr_y, tr_p = eval_loader(test_trans_loader, \"test_trans\")\n",
        "\n",
        "print(f\"ðŸŽ¯ TEST CIS   â†’ acc = {cis_acc:.3f}, macroF1 = {cis_mf1:.3f}\")\n",
        "print(f\"ðŸŽ¯ TEST TRANS â†’ acc = {tr_acc:.3f}, macroF1 = {tr_mf1:.3f}\\n\")\n",
        "\n",
        "print(\"--- CIS REPORT ---\")\n",
        "print(classification_report(cis_y, cis_p, target_names=[idx_to_class[i] for i in range(len(classes))]))\n",
        "\n",
        "print(\"\\n--- TRANS REPORT ---\")\n",
        "print(classification_report(tr_y, tr_p, target_names=[idx_to_class[i] for i in range(len(classes))]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-k58hFa5QIpj",
        "outputId": "61d2f56f-fdba-4c87-aa0c-ff0dfa78b552"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "FINAL TEST EVALUATION (Stage 1)\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_cis:   0%|          | 0/248 [00:00<?, ?it/s]/tmp/ipython-input-2929904351.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n",
            "test_trans:   0%|          | 0/364 [00:00<?, ?it/s]/tmp/ipython-input-2929904351.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n",
            "                                                             "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ¯ TEST CIS   â†’ acc = 0.921, macroF1 = 0.625\n",
            "ðŸŽ¯ TEST TRANS â†’ acc = 0.907, macroF1 = 0.600\n",
            "\n",
            "--- CIS REPORT ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      animal       0.93      0.99      0.96     13856\n",
            "         car       0.84      1.00      0.91       791\n",
            "       empty       0.17      0.00      0.00      1180\n",
            "\n",
            "    accuracy                           0.92     15827\n",
            "   macro avg       0.64      0.67      0.63     15827\n",
            "weighted avg       0.87      0.92      0.89     15827\n",
            "\n",
            "\n",
            "--- TRANS REPORT ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      animal       0.92      0.98      0.95     20384\n",
            "         car       0.73      1.00      0.85      1113\n",
            "       empty       0.04      0.00      0.00      1778\n",
            "\n",
            "    accuracy                           0.91     23275\n",
            "   macro avg       0.56      0.66      0.60     23275\n",
            "weighted avg       0.85      0.91      0.87     23275\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SAVE METADATA\n",
        "# ============================================================================\n",
        "import json\n",
        "\n",
        "mapping = {\n",
        "    \"classes\": classes,\n",
        "    \"class_to_idx\": class_to_idx\n",
        "}\n",
        "\n",
        "out_json = f\"{PROC_DIR}/stage1_label_mapping.json\"\n",
        "with open(out_json, \"w\") as f:\n",
        "    json.dump(mapping, f, indent=2)\n",
        "\n",
        "print(f\"\\nâœ“ Saved: {out_json}\")\n",
        "print(\"âœ“ Stage 1 training complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3SaOk7NQIsV",
        "outputId": "6616a29e-bb1e-4c03-fd84-53d49f94bf8b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ“ Saved: /content/drive/MyDrive/datasets/cct20/processed/stage1_label_mapping.json\n",
            "âœ“ Stage 1 training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9ASBJIXTRCU5"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}